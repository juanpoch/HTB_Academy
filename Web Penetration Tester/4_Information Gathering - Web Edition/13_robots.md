# robots.txt

Imagina que eres invitado a una gran fiesta en una mansi√≥n. Puedes recorrer la casa libremente, pero algunas habitaciones est√°n marcadas como "Privado" y se espera que no entres. De forma similar, el archivo **robots.txt** funciona como una gu√≠a de etiqueta para bots en el mundo web, indicando qu√© √°reas pueden explorar y cu√°les deber√≠an evitar.

---

# ¬øQu√© es robots.txt?

T√©cnicamente, **robots.txt** es un archivo de texto simple ubicado en el directorio ra√≠z de un sitio web.

Ejemplo:

```
https://www.example.com/robots.txt
```

Este archivo sigue el est√°ndar conocido como **Robots Exclusion Standard**, que define c√≥mo deben comportarse los crawlers al visitar un sitio web.

Contiene instrucciones llamadas **directivas**, que indican qu√© partes del sitio pueden o no pueden ser rastreadas.

---

# ¬øC√≥mo funciona robots.txt?

Las directivas dentro de robots.txt suelen dirigirse a **user-agents**, que identifican distintos tipos de bots.

Ejemplo b√°sico:

```
User-agent: *
Disallow: /private/
```

Esto significa:

* `User-agent: *` ‚Üí Aplica a todos los bots.
* `Disallow: /private/` ‚Üí No pueden acceder a URLs que comiencen con `/private/`.

Tambi√©n pueden:

* Permitir acceso a ciertas rutas.
* Definir retrasos entre solicitudes.
* Proporcionar enlaces a sitemaps.

---

# Estructura de robots.txt

El archivo robots.txt es un documento de texto plano ubicado en la ra√≠z del sitio.

Est√° compuesto por bloques llamados "records", separados por l√≠neas en blanco.

Cada bloque contiene:

## 1Ô∏è‚É£ User-agent

Especifica el bot al que aplican las reglas.

Ejemplos:

```
User-agent: *
User-agent: Googlebot
User-agent: Bingbot
```

---

## 2Ô∏è‚É£ Directivas

Son instrucciones espec√≠ficas para el user-agent definido.

### Directivas comunes

| Directive   | Description                                                                   | Example                                                                             |
| ----------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| Disallow    | Especifica rutas que el bot no debe rastrear.                                 | Disallow: /admin/                                                                   |
| Allow       | Permite expl√≠citamente ciertas rutas incluso si una regla Disallow las cubre. | Allow: /public/                                                                     |
| Crawl-delay | Define el tiempo de espera entre solicitudes.                                 | Crawl-delay: 10                                                                     |
| Sitemap     | Indica la URL del sitemap XML.                                                | Sitemap: [https://www.example.com/sitemap.xml](https://www.example.com/sitemap.xml) |

---

# ¬øPor qu√© respetar robots.txt?

Aunque robots.txt no es t√©cnicamente obligatorio (un bot malicioso podr√≠a ignorarlo), los crawlers leg√≠timos s√≠ lo respetan.

Respetarlo es importante por varias razones:

## Evitar sobrecargar el servidor

Limitar el acceso de bots puede prevenir tr√°fico excesivo que degrade el rendimiento.

## Proteger informaci√≥n sensible

Puede evitar que informaci√≥n privada sea indexada por motores de b√∫squeda.

## Cumplimiento legal y √©tico

Ignorar robots.txt podr√≠a violar t√©rminos de servicio o implicar problemas legales si se accede a datos privados o protegidos.

---

# robots.txt en Web Reconnaissance

Desde la perspectiva ofensiva, robots.txt es una fuente valiosa de inteligencia.

---

## üîé Descubrir directorios ocultos

Las rutas en `Disallow` suelen se√±alar:

* Paneles administrativos
* Directorios privados
* Backups
* Recursos sensibles

Parad√≥jicamente, lo que el administrador quiere ocultar a los buscadores puede convertirse en un punto de inter√©s para un atacante.

---

## üó∫ Mapear la estructura del sitio

El an√°lisis de rutas permitidas y denegadas ayuda a:

* Entender la organizaci√≥n interna.
* Detectar secciones no enlazadas desde el men√∫ principal.

---

## ü™§ Detectar trampas (Honeypots)

Algunos sitios incluyen rutas falsas en robots.txt para atraer bots maliciosos.

Identificar estas rutas puede indicar:

* Nivel de madurez defensiva.
* Presencia de mecanismos de monitoreo.

---

# Ejemplo de robots.txt

```
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml
```

### An√°lisis del ejemplo

* Todos los bots no pueden acceder a `/admin/` y `/private/`.
* Todos los bots pueden acceder a `/public/`.
* Googlebot debe esperar 10 segundos entre solicitudes.
* Se proporciona un sitemap para facilitar el rastreo.

---

# Inferencias desde Recon

A partir de este robots.txt podemos inferir:

* Existe un posible panel administrativo en `/admin/`.
* Hay contenido privado en `/private/`.
* Existe una estructura p√∫blica diferenciada (`/public/`).

Este tipo de informaci√≥n puede orientar etapas posteriores como:

* Directory enumeration
* Acceso manual a rutas interesantes
* An√°lisis de configuraci√≥n

---

# Conclusi√≥n

robots.txt es un archivo simple pero extremadamente informativo.

En reconocimiento web permite:

* Descubrir rutas interesantes.
* Comprender la estructura interna.
* Detectar posibles configuraciones inseguras.
* Obtener pistas sobre recursos sensibles.

Aunque est√° dise√±ado como gu√≠a para crawlers leg√≠timos, desde la perspectiva de seguridad puede revelar informaci√≥n estrat√©gica clave sobre la superficie de ataque del objetivo.
