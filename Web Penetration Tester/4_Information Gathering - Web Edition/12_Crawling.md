# Crawling

El **crawling**, tambiÃ©n conocido como *spidering*, es el proceso automatizado de navegaciÃ³n sistemÃ¡tica por la World Wide Web. De manera similar a cÃ³mo una araÃ±a recorre su telaraÃ±a, un web crawler sigue enlaces de una pÃ¡gina a otra recolectando informaciÃ³n.

Los crawlers son bots que utilizan algoritmos predefinidos para descubrir e indexar pÃ¡ginas web, ya sea para motores de bÃºsqueda, anÃ¡lisis de datos o procesos de reconocimiento en ciberseguridad.

---

# Â¿CÃ³mo funcionan los Web Crawlers?

El funcionamiento bÃ¡sico de un crawler es simple pero extremadamente poderoso:

1. Comienza con una **Seed URL** (URL inicial).
2. Descarga el contenido de esa pÃ¡gina.
3. Analiza el HTML.
4. Extrae todos los enlaces.
5. AÃ±ade esos enlaces a una cola.
6. Repite el proceso de manera iterativa.

Dependiendo de su configuraciÃ³n, puede:

* Explorar un sitio completo.
* Limitarse a un dominio especÃ­fico.
* Recorrer grandes porciones de la web.

---

## Ejemplo Conceptual

### Homepage inicial

```
Homepage
â”œâ”€â”€ link1
â”œâ”€â”€ link2
â””â”€â”€ link3
```

### Visitando link1

```
link1 Page
â”œâ”€â”€ Homepage
â”œâ”€â”€ link2
â”œâ”€â”€ link4
â””â”€â”€ link5
```

El crawler continÃºa siguiendo sistemÃ¡ticamente estos enlaces y recolectando todas las pÃ¡ginas accesibles.

âš  Diferencia clave:

* **Crawling** â†’ Sigue enlaces existentes.
* **Fuzzing** â†’ Intenta adivinar rutas potenciales.

---

# Estrategias de Crawling

Existen dos estrategias principales.

---

## 1ï¸âƒ£ Breadth-First Crawling (BFS)

Prioriza la anchura antes que la profundidad.

```
Seed
â”œâ”€â”€ Page 1
â”‚   â”œâ”€â”€ Page 2
â”‚   â””â”€â”€ Page 3
```

<img width="1234" height="783" alt="image" src="https://github.com/user-attachments/assets/235762de-44f3-4399-828d-e85ebf115766" />

El crawler:

* Explora todos los enlaces del nivel actual.
* Luego avanza al siguiente nivel.

### Ventajas

* Obtiene una visiÃ³n general rÃ¡pida del sitio.
* Ideal para mapear estructura.

---

## 2ï¸âƒ£ Depth-First Crawling (DFS)

Prioriza la profundidad antes que la anchura.

```
Seed
â””â”€â”€ Page 1
    â””â”€â”€ Page 2
        â””â”€â”€ Page 3
```

<img width="1268" height="222" alt="image" src="https://github.com/user-attachments/assets/ca280474-1cf7-4fa2-9cc2-a4aac2c1ae70" />

El crawler:

* Sigue un camino hasta el final.
* Luego retrocede y explora otras ramas.

### Ventajas

* Ãštil para llegar a contenido profundo.
* Ideal cuando se busca informaciÃ³n especÃ­fica.

---

La estrategia elegida depende del objetivo del reconocimiento.

---

# InformaciÃ³n Valiosa ExtraÃ­da mediante Crawling

Los crawlers pueden recolectar distintos tipos de datos crÃ­ticos para el reconocimiento:

---

## ğŸ”— Links (Internos y Externos)

* Mapeo completo del sitio.
* Descubrimiento de pÃ¡ginas ocultas.
* IdentificaciÃ³n de relaciones externas.

---

## ğŸ’¬ Comentarios

Los comentarios en:

* Blogs
* Foros
* CÃ³digo HTML

Pueden revelar:

* Procesos internos
* Versiones de software
* Pistas de vulnerabilidades

---

## ğŸ· Metadata

Incluye:

* TÃ­tulos
* Descripciones
* Keywords
* Autor
* Fechas

Proporciona contexto sobre el propÃ³sito y relevancia del contenido.

---

## ğŸ“‚ Archivos Sensibles

Un crawler puede detectar archivos expuestos como:

* `.bak`
* `.old`
* `web.config`
* `settings.php`
* `error_log`
* `access_log`

Estos pueden contener:

* Credenciales de base de datos
* API keys
* Claves de cifrado
* Fragmentos de cÃ³digo fuente

---

# La Importancia del Contexto

Un dato aislado puede parecer irrelevante, pero su valor aumenta cuando se correlaciona con otros hallazgos.

Ejemplo:

* Comentario menciona "file server".
* Crawling detecta mÃºltiples URLs en `/files/`.
* Se accede manualmente a `/files/`.
* Directory listing habilitado.
* Archivos sensibles expuestos.

La correlaciÃ³n convierte informaciÃ³n aparentemente trivial en un hallazgo crÃ­tico.

---

# AnÃ¡lisis HolÃ­stico

El verdadero valor del crawling no estÃ¡ en los datos individuales, sino en:

* Conectar patrones.
* Detectar relaciones.
* Identificar inconsistencias.
* Construir una imagen completa del entorno digital.

Un enfoque fragmentado puede pasar por alto vulnerabilidades crÃ­ticas.

---

# ConclusiÃ³n

El crawling es una tÃ©cnica fundamental en la fase de Information Gathering porque:

* Permite mapear la superficie de ataque real.
* Descubre recursos no documentados.
* Detecta configuraciones inseguras.
* Proporciona contexto para ataques dirigidos.

Cuando se combina con otras tÃ©cnicas como fingerprinting, CT logs y enumeraciÃ³n DNS, se convierte en una herramienta poderosa para comprender profundamente la infraestructura del objetivo.
