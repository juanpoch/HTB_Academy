# Creepy Crawlies

El mundo del web crawling es amplio y complejo, pero no es necesario recorrerlo manualmente. Existen m칰ltiples herramientas que automatizan el proceso de crawling, haci칠ndolo m치s r치pido y eficiente, permiti칠ndonos concentrarnos en el an치lisis de los datos extra칤dos.

---

# Popular Web Crawlers

## 游동 Burp Suite Spider

Burp Suite incluye un crawler activo llamado **Spider**.

Caracter칤sticas:

* Mapeo autom치tico de aplicaciones web.
* Descubrimiento de contenido oculto.
* Integraci칩n directa con herramientas de testing.

---

## 游띠 OWASP ZAP (Zed Attack Proxy)

ZAP es una herramienta gratuita y open-source.

Caracter칤sticas:

* Modo autom치tico y manual.
* Spider integrado.
* Identificaci칩n de vulnerabilidades comunes.

---

## 游냀 Scrapy (Framework en Python)

Scrapy es un framework potente y flexible para crear crawlers personalizados.

Caracter칤sticas:

* Extracci칩n estructurada de datos.
* Manejo de escenarios complejos.
* Automatizaci칩n de procesamiento.

Ideal para tareas de reconnaissance personalizadas.

---

## 游깷 Apache Nutch

Crawler open-source escalable escrito en Java.

Caracter칤sticas:

* Dise침ado para crawls masivos.
* Alta extensibilidad.
* Requiere mayor conocimiento t칠cnico.

M치s orientado a proyectos de gran escala.

---

# 칄tica y Responsabilidad

Siempre se debe:

* Obtener permiso antes de realizar crawling intensivo.
* Evitar sobrecargar el servidor.
* Respetar l칤mites de velocidad y pol칤ticas del sitio.

---

# Scrapy en Acci칩n

En este laboratorio utilizaremos **Scrapy** junto con un spider personalizado llamado **ReconSpider** para realizar reconnaissance sobre `inlanefreight.com`.

---

# Instalaci칩n de Scrapy

Si no est치 instalado:

```bash
pip3 install scrapy
```

En debian:
```bash
pipx install scrapy
```
Recomendaci칩n manual:
```bash
python3 -m venv recon-env
source recon-env/bin/activate
pip install scrapy
```

Esto instalar치 Scrapy y sus dependencias.

---

# Descargando ReconSpider

```bash
wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
unzip ReconSpider.zip
```

---

# Ejecutando el Spider

```bash
python3 ReconSpider.py http://inlanefreight.com
```

Reemplazar el dominio por el objetivo deseado.

El spider recorrer치 el sitio y recolectar치 informaci칩n estructurada.

---

# results.json

Tras la ejecuci칩n, se genera un archivo `results.json` con los datos extra칤dos.

Ejemplo de estructura:

```json
{
    "emails": ["lily.floid@inlanefreight.com"],
    "links": ["https://www.inlanefreight.com/index.php/offices/"],
    "external_files": ["https://www.inlanefreight.com/wp-content/uploads/2020/09/goals.pdf"],
    "js_files": ["https://www.inlanefreight.com/wp-includes/js/jquery/jquery-migrate.min.js"],
    "form_fields": [],
    "images": ["https://www.inlanefreight.com/wp-content/uploads/2021/03/AboutUs.png"],
    "videos": [],
    "audio": [],
    "comments": ["<!-- #masthead -->"]
}
```

---

# Significado de cada clave

| JSON Key       | Descripci칩n                                     |
| -------------- | ----------------------------------------------- |
| emails         | Direcciones de correo encontradas en el dominio |
| links          | URLs internas encontradas                       |
| external_files | Archivos externos como PDFs                     |
| js_files       | Archivos JavaScript utilizados                  |
| form_fields    | Campos de formularios detectados                |
| images         | URLs de im치genes                                |
| videos         | URLs de videos                                  |
| audio          | URLs de audio                                   |
| comments       | Comentarios HTML encontrados                    |

---

# Valor en Reconnaissance

Analizando este JSON podemos:

* Identificar correos para OSINT.
* Detectar archivos interesantes (PDFs, backups).
* Enumerar librer칤as JS vulnerables.
* Encontrar endpoints ocultos.
* Extraer comentarios potencialmente sensibles.

---

# Conclusi칩n

Las herramientas de crawling automatizado permiten:

* Acelerar la fase de reconocimiento.
* Extraer datos estructurados.
* Descubrir recursos ocultos.
* Priorizar vectores de ataque.

El verdadero valor no est치 solo en recolectar datos, sino en analizarlos y correlacionarlos con otros hallazgos como fingerprinting, robots.txt y .well-known endpoints.




---


# Preguntas


#### Despu칠s de rastrear inlanefreight.com, identifique la ubicaci칩n donde se almacenar치n los informes futuros. Responda con el dominio completo, por ejemplo, files.inlanefreight.com.

`Pista`: Quiz치s haya un comentario al respecto.
